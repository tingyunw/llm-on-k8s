version: '3.8'

services:
  llm-frontend:
    build:
      context: ./llm-all-in-one
      dockerfile: Dockerfile
    ports:
      - "8501:8501"
    depends_on:
      - llm-backend
    command: ["streamlit", "run", "pipelines/frontend/main.py", "--server.port=8501", "--server.address=0.0.0.0"] 
    networks:
      - llm-network

  llm-backend:
    build:
      context: ./llm-all-in-one
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    command: ["uvicorn", "pipelines.server.routers.main:app", "--host", "0.0.0.0", "--port", "8080"] 
    networks:
      - llm-network
  
  redis:
    image: redis:7.2-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    networks:
      - llm-network
  
  flower:
    build:
      context: ./llm-all-in-one
      dockerfile: Dockerfile
    ports: 
      - "5555:5555"
    command: celery -A pipelines.server.celery_app.app flower --address=0.0.0.0
    networks:
      - llm-network
  
  celery-worker1:
    build:
      context: ./llm-all-in-one
      dockerfile: Dockerfile
    environment:
      - OLLAMA_HOST=http://ollama1:11434
    command: celery -A pipelines.server.celery_app.app worker --loglevel=info
    networks:
      - llm-network
  
  celery-worker2:
    build:
      context: ./llm-all-in-one
      dockerfile: Dockerfile
    environment:
      - OLLAMA_HOST=http://ollama2:11434
    command: celery -A pipelines.server.celery_app.app worker --loglevel=info
    networks:
      - llm-network
  
  ollama1:
    build:
      context: ./llm-model
      dockerfile: Dockerfile
    networks:
      - llm-network
  ollama2:
    build:
      context: ./llm-model
      dockerfile: Dockerfile
    networks:
      - llm-network
  
  node-exporter:
    image: prom/node-exporter:v1.9.1
    container_name: node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
    restart: unless-stopped
    networks:
      - llm-network
    
  prometheus:
    image: prom/prometheus:v3.4.0
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./files/prometheus.yml:/etc/prometheus/prometheus.yml
    networks:
      - llm-network
  
  grafana:
    image: grafana/grafana:12.0.1
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana-storage:/var/lib/grafana
    networks:
      - llm-network
  
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.49.1
    container_name: cadvisor
    restart: unless-stopped
    ports:
      - "8081:8080"  # Expose the cAdvisor web UI on host port 8081
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - llm-network

networks:
  llm-network:
    driver: bridge

volumes:
  redis-data:
    driver: local 
  grafana-storage:
    driver: local